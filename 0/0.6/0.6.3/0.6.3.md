> [0. Acerca del Grupo](../../0.md) ‚Ä∫ [0.6. Temas Individuales (Parte 1)](../0.6.md) ‚Ä∫ [0.6.3. Integrante 3](0.6.3.md)

# 0.6.3. Integrante 3

# An√°lisis y Visualizaci√≥n de Logs

## 1.Desarrollo conceptual

### ¬øQu√© es el an√°lisis de logs?

El an√°lisis de logs es el proceso de recopilar, procesar e interpretar los registros generados por sistemas, aplicaciones o dispositivos para obtener informaci√≥n √∫til.
Los logs contienen eventos que describen lo que ocurre dentro de una infraestructura tecnol√≥gica (por ejemplo, conexiones de red, errores de servidor, autenticaciones, o tr√°fico sospechoso).

Analizar estos datos permite:

- Detectar incidentes de seguridad.

- Identificar patrones de uso o fallas recurrentes.

- Medir el rendimiento de servicios.

- Predecir comportamientos an√≥malos mediante modelos de Machine Learning.

### ¬øQu√© es la visualizaci√≥n de logs?

La visualizaci√≥n de logs consiste en representar gr√°ficamente los datos procesados para facilitar su interpretaci√≥n.
Mediante gr√°ficos de l√≠neas, barras, mapas o indicadores, los analistas pueden identificar tendencias, picos de actividad o comportamientos an√≥malos de forma inmediata.

### Ejemplo:

>En lugar de revisar miles de l√≠neas de texto en un archivo de log, un gr√°fico puede mostrar c√≥mo el tr√°fico de red aument√≥ repentinamente en una hora espec√≠fica, indicando un posible ataque.

---

### Tipos de Logs

Existen m√∫ltiples tipos de logs, seg√∫n su origen y prop√≥sito:

| Tipo de Log | Descripci√≥n | Ejemplo |
|--------------|-------------|----------|
| **Logs del sistema** | Registran actividades del sistema operativo, como inicio de servicios o errores del kernel. | `syslog`, `dmesg` |
| **Logs de aplicaci√≥n** | Capturan eventos dentro de una aplicaci√≥n, como errores, accesos o transacciones. | `error.log`, `access.log` |
| **Logs de red** | Monitorean el tr√°fico, conexiones y paquetes transmitidos entre dispositivos. | `firewall.log`, `network_flows.csv` |
| **Logs de seguridad** | Detectan intentos de acceso, autenticaciones fallidas o posibles amenazas. | `auth.log`, `IDS/IPS logs` |

---

### Retos del an√°lisis de logs

**1. Volumen masivo de datos**

Los sistemas modernos generan millones de registros cada d√≠a, provenientes de servidores, aplicaciones y dispositivos de red. Esto dificulta su almacenamiento, b√∫squeda y procesamiento en tiempo real.

**2. Estructura poco uniforme**

Cada fuente de log puede tener un formato distinto (JSON, texto plano, CSV, XML, etc.). Esta falta de estandarizaci√≥n complica la integraci√≥n y el an√°lisis automatizado de la informaci√≥n.

**3. Ruido o informaci√≥n irrelevante**

Dentro de los logs hay gran cantidad de mensajes que no aportan valor al an√°lisis (informaci√≥n repetitiva o trivial). Filtrar este ruido es esencial para centrarse en los eventos cr√≠ticos o an√≥malos.

**4. Detecci√≥n de patrones y anomal√≠as**

Identificar comportamientos inusuales, ataques o errores requiere t√©cnicas avanzadas de correlaci√≥n y machine learning. A simple vista, los patrones importantes pueden pasar desapercibidos.

--- 

### Proceso del An√°lisis de Logs

El an√°lisis de logs sigue una serie de etapas que permiten transformar registros  en informaci√≥n √∫til para la toma de decisiones o la detecci√≥n de problemas.

**1. Recolecci√≥n**

En esta etapa se capturan los logs desde diferentes fuentes. El objetivo es centralizarlos en un solo punto para facilitar su gesti√≥n.

**2. Procesamiento**

Aqu√≠ los logs se limpian, transforman y enriquecen.

- Se eliminan l√≠neas irrelevantes.
- Se corrige el formato.
- Se agregan datos adicionales, como etiquetas, origen o nivel de severidad.

**3. Almacenamiento**

Los logs ya procesados se guardan en una base de datos optimizada para b√∫squedas r√°pidas.
Esto permite hacer consultas complejas incluso sobre millones de registros.

**4. An√°lisis**

En esta fase se exploran los datos, se ejecutan consultas y se buscan patrones o anomal√≠as.
El an√°lisis puede ser manual o automatizado.

**5. Visualizaci√≥n**

Finalmente, los resultados se presentan mediante gr√°ficos, dashboards o reportes que facilitan la interpretaci√≥n.
Esto permite detectar r√°pidamente problemas o tendencias sin tener que revisar l√≠neas de texto.

### La Pila ELK (Elasticsearch ‚Äì Logstash ‚Äì Kibana)

La **pila ELK** es una de las soluciones m√°s populares y completas para el **an√°lisis y visualizaci√≥n de logs**.

| **Componente** | **Funci√≥n**                  | **Descripci√≥n** |
|-----------------|------------------------------|-----------------|
| **Logstash**    | Ingesta y procesamiento       | Recibe los logs, aplica filtros (CSV, JSON, GROK), limpia los datos y los env√≠a a Elasticsearch. |
| **Elasticsearch** | Almacenamiento e indexaci√≥n | Base de datos optimizada para b√∫squedas y consultas en tiempo real. |
| **Kibana**      | Visualizaci√≥n                | Interfaz gr√°fica para explorar, analizar y crear dashboards interactivos. |


### Arquitectura general del ELK Stack

La siguiente imagen muestra c√≥mo se integran los tres componentes principales del **ELK Stack**. 

Cada servidor o aplicaci√≥n **env√≠a sus logs** a **Logstash**, que los **filtra y transforma** antes de enviarlos a **Elasticsearch**, donde se **indexan y almacenan**.  
Finalmente, **Kibana** permite **visualizar y analizar** esa informaci√≥n.
![Diagrama ELK stack](./elk-logs-demo/Images/ELKstack.png)

---

### visualizaci√≥n de logs en Kibana

Una vez que los datos son procesados por Logstash y almacenados en Elasticsearch, la herramienta Kibana permite explorar y representar visualmente la informaci√≥n.
Kibana ofrece paneles interactivos, filtros din√°micos y la posibilidad de realizar b√∫squedas personalizadas sobre los campos de los registros.

---

### B√∫squeda de datos con KQL

KQL (Kibana Query Language) es el lenguaje de consulta utilizado dentro de Kibana para filtrar y explorar datos almacenados en Elasticsearch.
Permite realizar b√∫squedas avanzadas sobre los campos de los documentos, combinando condiciones y operadores l√≥gicos.

---


## 2. Consideraciones t√©cnicas  

### Arquitectura general  

La arquitectura del proyecto se basa en la **Pila ELK (Elasticsearch, Logstash y Kibana)**, desplegada dentro de contenedores Docker para garantizar portabilidad, facilidad de instalaci√≥n y aislamiento entre servicios.  
Todo el entorno se orquesta mediante **Docker Compose**, lo que permite iniciar o detener toda la pila con un solo comando.  

---

### Requisitos previos  

Antes de ejecutar la demo, es necesario cumplir con los siguientes requisitos t√©cnicos:  

1. **Instalar Docker y Docker Compose**  
   - Permiten desplegar los tres servicios en contenedores independientes sin necesidad de instalaci√≥n manual.  
   - Docker Compose simplifica el proceso de ejecuci√≥n con un √∫nico archivo `docker-compose.yml`.  

2. **Contar con un archivo de logs**  
   - En este caso se utiliza `network_logs.csv`, el cual contiene registros de tr√°fico de red, sin embargo se pueden utilizar varios archivos.

3. **Configuraci√≥n personalizada de Logstash**  
   - Se define en un archivo `.conf` donde se especifican:  
     - La ruta del archivo de entrada.  
     - El formato CSV y sus columnas.  
     - Filtros para normalizar datos.  
     - El destino de salida hacia Elasticsearch.  

4. **Definir un √≠ndice en Elasticsearch**  
   - Los datos procesados se almacenan bajo un patr√≥n, lo que permite agrupar los registros por fecha y facilitar su b√∫squeda posterior.  

# 3. Demo (C√≥digo)

## `docker-compose.yml`
Este archivo define la infraestructura del entorno ELK (Elasticsearch, Logstash y Kibana) utilizando **Docker Compose**.  
Cada servicio se ejecuta en un contenedor independiente, lo que facilita la configuraci√≥n, despliegue y mantenimiento del sistema.

```yaml
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.15.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - es_data:/usr/share/elasticsearch/data

  kibana:
    image: docker.elastic.co/kibana/kibana:8.15.0
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200

  logstash:
    image: docker.elastic.co/logstash/logstash:8.15.0
    container_name: logstash
    volumes:
      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
      - ./logs:/usr/share/logstash/logs
    ports:
      - "5044:5044"
      - "9600:9600"

volumes:
  es_data:
```


## `logstash.conf`
Define c√≥mo Logstash procesa y env√≠a los datos de los registros hacia Elasticsearch.
Este archivo se encuentra en la carpeta del proyecto y se ejecuta autom√°ticamente al levantar los contenedores con Docker Compose.

Este archivo realiza tres tareas principales:

| Etapa   | Descripci√≥n |
|----------|-------------|
| **Input** | Indica a Logstash de d√≥nde leer los datos. En este caso, de un archivo CSV (`network_logs.csv`). |
| **Filter** | Limpia, transforma y normaliza los datos: convierte tipos de datos, ajusta el formato de fecha y define el campo temporal `@timestamp`. |
| **Output** | Env√≠a los datos procesados a **Elasticsearch**, creando √≠ndices con el formato `network-flows-YYYY.MM.dd`. |




```ruby
input {
  file {
    path => "/usr/share/logstash/logs/network_logs.csv"
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

filter {
  csv {
    separator => ","
    skip_header => "true"
    columns => [
      "date_first_seen","duration","proto","src_ip","src_port",
      "dst_ip","dst_port","packets","bytes","flows","flags",
      "tos","class","attackType","attackID","attackDescription"
    ]
  }

  # Normalizamos el campo de tiempo -> lo pasamos a @timestamp
  date {
    match => ["date_first_seen", "yyyy-MM-dd HH:mm:ss.SSS"]
    target => "@timestamp"
  }

  # Convertimos m√©tricas a num√©ricas
  mutate {
    convert => {
      "duration" => "float"
      "packets" => "integer"
      "bytes" => "integer"
      "flows" => "integer"
    }
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "network-flows-%{+YYYY.MM.dd}"
  }
  stdout { codec => rubydebug }
}
```

## Importar el Dashboard en Kibana

Para visualizar la **demo del sistema de an√°lisis y visualizaci√≥n de logs** con el mismo dashboard que se cre√≥ en el entorno original, es necesario importar el archivo exportado desde Kibana.

####  Pasos para importar el Dashboard:

1. Aseg√∫rate de tener los contenedores del entorno ELK en ejecuci√≥n.

2. Abre Kibana en tu navegador.

3. Dentro de Kibana. En el men√∫ lateral, entra a:
Stack Management ‚Üí Saved Objects

4. Haz clic en el bot√≥n Import

5. Selecciona el archivo `.ndjson` que se encuentra en la carpeta del proyecto.

6. Una vez completada la importaci√≥n, ve a:
Analytics ‚Üí Dashboard,
y abre el dashboard importado para visualizar los gr√°ficos y m√©tricas de la demo.

## Ejemplo del Dashboard en Ejecuci√≥n

A continuaci√≥n, se presentan capturas del **dashboard importado en Kibana**, las cuales muestran c√≥mo se visualizan los datos procesados por la pila **ELK (Elasticsearch, Logstash y Kibana)**.

Estas im√°genes ilustran los diferentes tipos de gr√°ficos generados a partir de los registros de red, permitiendo observar **tendencias, vol√∫menes de tr√°fico y comportamiento de las conexiones**.

![Demo 1](./elk-logs-demo/Images/Demo.png)

![Demo 2](./elk-logs-demo/Images/Demo1.png)

## Utiliza KQL

KQL permite combinar condiciones l√≥gicas, operadores de comparaci√≥n y b√∫squedas por campos, ofreciendo una gran flexibilidad para el an√°lisis.
Para realizar estas consultas, se debe seguir el siguiente procedimiento:

1. Abre Kibana y accede a la secci√≥n Discover desde el men√∫ lateral.

2. En la parte superior, selecciona el Data View correspondiente.

3. En la barra de b√∫squeda de la parte superior, escribe tus consultas KQL.

4. Presiona Enter para aplicar el filtro y ver los resultados en tiempo real.

### Ejemplos 

**Ejemplo 1.** Mostrar solo los registros donde el campo class tiene el valor normal.

```kql
class.keyword : "normal"
```
![Ejemplo1](./elk-logs-demo/Images/Ejemplo1.png)

**Ejemplo 2.** Muestra los registros donde el puerto de destino es mayor o igual a 80 y el n√∫mero de paquetes es superior a 100.

```kql
dst_port >= 80 and packets > 100
```
![Ejemplo2](./elk-logs-demo/Images/Ejemplo2.png)

**Ejemplo 3.** Muestra los eventos que transfirieron m√°s de 100,000 bytes.

```kql
bytes > 100000
```

![Ejemplo3](./elk-logs-demo/Images/Ejemplo3.png)

**Ejemplo 4.** 
Detectar sesiones largas o con tr√°fico elevado.

```kql
packets > 1000 and duration > 10
```

![Ejemplo4](./elk-logs-demo/Images/Ejemplo4.png)

**Ejemplo 5.** Busca los registros de tr√°fico sospechoso (suspicious) donde los bytes transferidos sean mayores a 5000, y que adem√°s el protocolo (proto) sea TCP o UDP.

```kql
class.keyword : "suspicious" and bytes > 5000 and (proto : "TCP" or proto: "UDP")
```
![Ejemplo5](./elk-logs-demo/Images/Ejemplo5.png)


## Video Demo
Puedes ver el video de la demo aqu√≠: 
**Drive:**
[Ver video de la demo](https://drive.google.com/file/d/1PgKv4rYYz5z8470hGiJaUQul7AjVAeBB/view?usp=sharing)

**Youtube**
[Ver video de la demo](https://drive.google.com/file/d/1PgKv4rYYz5z8470hGiJaUQul7AjVAeBB/view?usp=sharing)

---

[‚¨ÖÔ∏è Anterior](../0.6.2/0.6.2.md) | [üè† Home](../../../README.md) | [Siguiente ‚û°Ô∏è](../0.6.4/0.6.4.md)